\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Week 1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Linear Regression - Model and Cost Function}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Univariate Linear Regression Hypothesis}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Cost Function}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Cost Function Intuition}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Gradient Descent}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Intuition}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Week 2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Multivariate Linear Regression}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Multiple Features}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Gradient Descent for Multiple Variables ($n \geq 1$)}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Feature Scaling}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Learning Rate}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}Polynomial Regression}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Computing Parameters Analytically - Normal Equation}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Gradient Descent vs Normal Equation}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Normal Equation Non-invertibility}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Vectorization of Implementation}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Week 3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Classification}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Hypothesis Representation}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Decision Boundary}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Logistic Regression Model}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Cost Function}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Gradient Descent}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Advanced Optimization}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Multi-class Classification (any number of outputs)}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}The Problem of Over-fitting}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Addressing Overfitting}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Regularization}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3}Regularized Linear Regression}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.4}Regularized Logistic Regression}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Week 4}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Neural Networks}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Model Representation}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Applications}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Multiclass Classification}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Week 5}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Neural Network Cost Function}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Gradient Computation}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Intuition on Back Propagation}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Back Propagation in Practice}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Gradient Checking}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Initialization of $\theta $ Parameters}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Putting it all Together}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Week 6}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Evaluating a Learning Algorithm}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Evaluating Hypothesis}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Model Selection - How to Choose the Degree of Polynomial to Fit}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Bias vs. Variance}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Diagnosing Bias vs Variance}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Auto-selecting $\lambda $}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.3}Learning Curves}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.4}Relating to Neural Networks}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Spam Classifier}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Error Analysis}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Handling Skewed Data}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.1}Precision/recall}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.2}Trading Off Precision and Recall}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Using Large Data Sets}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Week 7}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Large Margin Classification - Support Vector Machines}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Optimization Objective}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Large Margin Intuition}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Kernels}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}Motivation}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}Landmarks}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.3}Kernel Function}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.4}How to Choose Landmarks}{21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.5}SVM with Kernels}{21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.6}SVM Parameters}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Using an SVM}{21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Other Kernels}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Multi-class Classification}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}Logistic Regression vs SVM}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.4}Neural Network vs SVM}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Week 8}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Clustering - K-means Algorithm}{23}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1}K-means for non-separated clusters}{23}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2}Optimization Objective}{23}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.3}Random Initialization}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.4}Choosing Number of Clusters}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Dimensionality Reduction}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}Principal Component Analysis (PCA)}{25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}PCA Algorithm}{25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.3}Reconstruction from Compressed Representation}{26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.4}Choosing Number of Principal Components $k$}{26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.5}Advice for Applying PCA}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Anomaly Detection}{27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.1}Gaussian Distribution}{27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.2}Density Estimation}{27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.3}Developing and Evaluating an Anomaly Detection System}{27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.4}Anomaly Detection vs Supervised Learning}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.5}Choosing what Features to Use for Anomaly Detection}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.6}Multivariate Gaussian Distribution}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.7}Fitting Parameters $\mu $ and $\DOTSB \sum@ \slimits@ $}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.8}Relationship to Original Model}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.9}Anomaly Detection Algorithm with Multivariate Gaussian}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.10}When to use Multivariate vs Original Model}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Recommendation Systems}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.1}Problem formulation}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.2}Content-based Recommendations}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Collaborative Learning}{31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.1}Optimization algorithm}{31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.5.2}Collaborative Filtering Algorithm}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6}Low Rank Matrix Factorization}{33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.6.1}Vectorization: Low Rank Matrix Factorization}{33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.6.2}Finding Related Movies}{33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.6.3}Implementation Detail - Mean Normalization}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Week 10}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Learning With Large Datasets}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Gradient Descent with Large Datasets}{33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.1}Stochastic Gradient Descent}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.2}Mini-Batch Gradient Descent}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.3}Stochastic Gradient Descent Convergence}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Online Learning}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Map Reduce and Data Parallelism}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Week 11}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Photo OCR}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Sliding Windows}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Getting Lots of Data and Artificial Data}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Ceiling Analysis: What Part of Pipeline to Work on Next}{37}}
